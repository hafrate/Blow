---
title: Pr√©vision de d√©faut du paiement avec Xgboost
author: 'Mohammed Hafrate'
date: '2022-04-10'
categories: 
   - Power query
   - ODBC
   - Sqlite
   - Power BI
tags: ["Sqlite", "ODBC", "Power BI"]

summary: "Step by step du framework tidymodel pour pr√©voir les Client avec un d√©fault de paiement par un mod√®le de classification 'Xgboost)"
thumbnailImage: "/img/DAX/sqlite_bi.png"
thumbnailImagePosition: left
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, tidy = "styler")
```

Dans ce post, nous allons entra√Æner et le r√©gler les hyperparam√®tres du mod√®le XGBoost.
On va utiliser le package **tidymodels**, qui se compose de plusieurs library, un workflow pour entrainer les models de classification (ou r√©gression) :
  
  - **recipes** : framework pour le Preprocessing
- **rsample** : Cross Validation et sample 
- **parsnip** : framework de train des Machines Learnings
- **tune** : Tuning des hyperparam√®tres
- **yardstick** : evaluation sur les donn√©es Test

Ce projet consiste √† construire la meilleure fonction score possible sur une base de donn√©es classique en machine learning : la base Credit Data disponible dane le package *modeldata*. 

Load packages necessaires

```{r}
# Load les library	
library(tidymodels)  
library(dplyr)
library(modeldata)
# for EDA
library(summarytools) 
library(skimr)
library(DataExplorer)
# Helper packages
library(modeldata)   # for Data score
library(vip) 
# Parallel processing 
library(doParallel)
```


## Load les donn√©es

```{r}
# set random seed for reproduction
set.seed(123)
data("credit_data", package = "modeldata")
credit_data %>% glimpse()
```

# Exploaration des donn√©es - EDA avec skimr

```{r}
skim(credit_data)
```

# Splitting -- la r√©partition des donn√©es
Maintenant, on divise les donn√©es en donn√©es de validation et de test. Les donn√©es de validation sont utilis√©es pour l'entrainement du mod√®le et l‚Äôajustement des hyperparam√®tres. Une fois le mod√®le construit, il peut √™tre √©valu√© par rapport aux donn√©es test pour √©valuer la qualit√© et la pr√©cision du nouveau mod√®le.


```{r}
split <- initial_split(credit_data, prop = 0.8, strata = Status)
split
train_data <- training(split)
test_data <-  testing(split)
```

# Preprocessing
Le pr√©traitement modifie les donn√©es pour rendre notre mod√®le plus pr√©dictif et le processus de formation moins intensif. De nombreux mod√®les n√©cessitent un pr√©traitement minutieux et exhaustif des variables pour produire des pr√©visions pr√©cises.
XGBoost, cependant, est robuste contre les donn√©es fortement asym√©triques et/ou corr√©l√©es. N√©anmoins, nous pouvons encore b√©n√©ficier d‚Äôun certain pr√©traitement 

Dans tidymodels, nous utilisons **recipes** pour d√©finir ces √©tapes de pr√©traitement.
Le preprocessing a √©t√© d√©ini au d√©part pour entrainer le mod√©le **Catboost**


```{r}
recipe_credit <- train_data %>% recipe(Status ~ .) %>% 
  step_unknown(all_nominal(), -Status) %>% 
  step_medianimpute(all_numeric()) %>% 
  step_other(all_nominal(), - Status, other = "infrequent_combined") %>% 
  step_novel(all_nominal(), - Status, new_level = "unrecorded_observation") %>% 
  step_dummy(all_nominal(), -Status, one_hot = TRUE) %>%
  step_mutate(ratio_expense_income = Expenses/ (Income + 0.001 ),
              ratio_asset_income = Assets/ (Income + 0.001),
              ratio_debt_asset = Debt / (Assets + 0.001),
              ratio_debt_income =Debt /(Income +0.001),
              ratio_amount_price = Amount / (Price +0.010)) 
  # Add upsmpling
#  step_upsample(Status, over_ratio = tune())
recipe_credit
```

# Splitting pour la validation crois√©e

```{r}
cv_fold <-  vfold_cv(train_data, v =5, strata = Status) 
cv_fold
```

# Specifications du mod√©le XGboost
On utilise le package **parsnip** pour d√©finir la sp√©cification du mod√®le. Parsnip permet d‚Äôacc√©der √† plusieurs packages d‚Äôapprentissage automatique
les 3 √©tapes:
  1- d√©finir le type du mod√®le √† entrainer 
  2- D√©cider quel moteur (engine) de calcul √† utiliser
  3- d√©finir le mode "classification"

```{r}
xgb_spec <- boost_tree( trees = tune(),
                        tree_depth = tune(), min_n = tune(), 
						loss_reduction = tune (),
						sample_size = tune(), mtry = tune(),
						learn_rate = tune()) %>% 
  set_engine("xgboost")%>% 
  set_mode("classification")
xgb_spec %>%  translate ()
```

#### Grid specification
On va utiliser `grid_latin_hypercube` afin que nous puissions couvrir l‚Äôespace des hyperparam√®tres le mieux possible
```{r}
xgb_grid <- grid_latin_hypercube ( 
        trees(),
				tree_depth (),
				min_n(),
				loss_reduction (),
				sample_size = sample_prop(),
				finalize(mtry(), train_data),
				learn_rate(),
				size = 50)
xgb_grid
```

# D√©finir workflow

```{r}

xgb_wf <- workflow()%>%
			add_recipe(recipe_credit)%>%
			add_model(xgb_spec) 
    
xgb_wf
```

# Tune des hyperparam√®tres
Pour acc√©l√©rer le calcul, on utilise paralell processing. Pour en savoir plus https://curso-r.github.io/treesnip/articles/parallel-processing.html

```{r}
all_cores <- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = all_cores)
set.seed(452)
library(tictoc)
cat("Nombre de cores :" , all_cores)

```

Le tuning prends un peu de temps pour √©valuer l'ensemble des param√©tres.
```{r}
tic()
xgb_tune <- tune_grid(
  xgb_wf,
  resamples = cv_fold,
  #metrics = metric_set(roc_auc, j_index, sens, spec),
  control = control_grid(verbose = FALSE, save_pred = TRUE))
toc ()
xgb_tune
```

# Evaluer le R√©sultat
On va sortir les param√®tres de tous ces mod√®les.
```{r}
DT::datatable(xgb_tune%>% collect_metrics( ))
```

ROC curve + gain curve

```{r}
library(patchwork)

p1 <- xgb_tune %>% collect_predictions() %>% 
  group_by(.config) %>% 
  roc_curve(Status, .pred_bad) %>% 
  autoplot() + theme(legend.position = "none")

p2 <- xgb_tune %>% collect_predictions() %>% 
  group_by(.config) %>% 
  gain_curve(Status, .pred_bad) %>% 
  autoplot()+ theme(legend.position = "none")

#p1 / p2
p1 + p2

```

Quels sont les combinaisons de param√®tres les plus performants?
Et, Finaliser le mod√®le XGBoost avec les meilleurs param√®tres.

```{r}
xgb_best <-	select_best(xgb_tune, "roc_auc")
xgb_final_wf <- finalize_workflow(xgb_wf, xgb_best)
xgb_best
```


```{r}
xgb_tune%>% 
  collect_metrics(parameters = xgb_best)
```


What are the most important parameters for variable importance?
  
```{r}
library(vip)

xgb_final_wf %>%
  fit(data = train_data) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")
```


last_fit() pour adapter notre mod√®le une derni√®re fois sur les donn√©es de validation et √©valuer notre mod√®le sur les donn√©es de tests

```{r}
xgb_final<- xgb_final_wf %>%
  last_fit( split,  metrics =metric_set(roc_auc, j_index, sens, spec, accuracy))
xgb_final %>%  collect_metrics()
```

La courbe ROC nous donnera une id√©e de la performance de notre mod√®le avec les donn√©es tests. Vous devriez savoir maintenant que si AUC (Area Under Curve) est proche de 50%, alors le mod√®le est aussi bon qu‚Äôun s√©lection al√©atoire; par contre, si AUC est proche de 100%, vous avez un ¬´mod√®le parfait¬ª

```{r}
xgb_final %>%
  collect_predictions() %>% 
  roc_curve(Status,  .pred_bad) %>% 
  autoplot()

xgb_final %>%
  collect_predictions()%>% 
  roc_curve(Status, .pred_bad) %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) + 
  geom_path(lwd = 1.5, alpha = 0.8, color = "#421269") +
  geom_abline(lty = 3) + 
  geom_hline(yintercept =0.42)+
  geom_vline(xintercept =1-0.9186228)+
  coord_equal() + 
  #scale_color_viridis_d(option = "plasma", end = .6)+
   theme_bw()
```
Analyse de la densit√© pour chaque classe  vs pr√©vision, ce graphe permet de fournir des informations visuelles sur l‚Äôasym√©trie, la distribution, et la qualit√© de pr√©vision de notre mod√®le.
```{r}
xgb_auc <- xgb_final%>%collect_predictions()
h1 <- xgb_auc %>% 
  ggplot()+
  geom_density( aes(x=.pred_bad, fill = Status), alpha =0.5) + theme_bw()
h2 <- xgb_auc %>% 
  ggplot()+
  geom_density( aes(x=.pred_good,fill = Status), alpha =0.5)+ theme_bw()
h1/h2
```

La performance du mod√®le XGboost n'est aussi pas optimale qu‚Äôattendue...Il faut revoir le process, peut-√™tre en incr√©mentant d'autres donn√©es, revoir notre strat√©gie du tuning et du preprocessing, chnager le thresholds ou changer le mod√®le comme le Catboost ou LightGBM ; il y a toujours des pistes üòä
